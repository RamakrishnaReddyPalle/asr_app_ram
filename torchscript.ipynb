{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e45f2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nemo.collections.asr.models import EncDecCTCModel\n",
    "import torchaudio\n",
    "from itertools import product\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import time, onnxruntime, numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0fe85e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-24 17:37:25 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-24 17:37:25 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /code/03_manifest/ulca-train-v3.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 16.7\n",
      "    min_duration: 0.1\n",
      "    \n",
      "[NeMo W 2025-05-24 17:37:25 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2025-05-24 17:37:25 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-24 17:37:25 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2025-05-24 17:37:26 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from d:\\IIT BBS\\Job Resources\\mvaakAI\\asr_app\\model\\stt_hi_conformer_ctc_medium.nemo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncDecCTCModelBPE(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): ConformerEncoder(\n",
       "    (pre_encode): ConvSubsampling(\n",
       "      (out): Linear(in_features=5120, out_features=256, bias=True)\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_enc): RelPositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x ConformerLayer(\n",
       "        (norm_feed_forward1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward1): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): ConformerConvolution(\n",
       "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (depthwise_conv): CausalConv1D(256, 256, kernel_size=(31,), stride=(1,), groups=256)\n",
       "          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): Swish()\n",
       "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (norm_self_att): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): RelPositionMultiHeadAttention(\n",
       "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (norm_feed_forward2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward2): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvASRDecoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Conv1d(256, 129, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (loss): CTCLoss()\n",
       "  (spec_augmentation): SpectrogramAugmentation(\n",
       "    (spec_augment): SpecAugment()\n",
       "  )\n",
       "  (wer): WER()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load & configure model ---\n",
    "asr_model = EncDecCTCModel.restore_from(\"model/stt_hi_conformer_ctc_medium.nemo\").eval()\n",
    "asr_model.preprocessor.featurizer.dither = 0.0\n",
    "asr_model.preprocessor.featurizer.pad_to = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "asr_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a05004ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load & preprocess audio ---\n",
    "wav_path = \"test/test1.wav\"\n",
    "waveform, sr = torchaudio.load(wav_path)\n",
    "if sr != 16000:\n",
    "    waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
    "\n",
    "# Mix to mono & reshape\n",
    "if waveform.size(0) > 1:\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "waveform = waveform.squeeze(0)  # [1, N] → [N]\n",
    "audio_input = waveform.unsqueeze(0).to(device)  # [1, N]\n",
    "length = torch.tensor([audio_input.shape[1]], dtype=torch.int64).to(audio_input.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8c6245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Updated Wrapper with encoder and decoder separation ---\n",
    "class ASRWrapper(nn.Module):\n",
    "    def __init__(self, nemo_model):\n",
    "        super().__init__()\n",
    "        self.model = nemo_model\n",
    "\n",
    "    def forward(self, input_signal, input_signal_length):\n",
    "        return self.model(input_signal=input_signal, input_signal_length=input_signal_length)\n",
    "        \n",
    "wrapper = ASRWrapper(asr_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7ff3c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 4\n"
     ]
    }
   ],
   "source": [
    "# --- Parameter grid ---\n",
    "param_space = {\n",
    "    \"export_params\":      [True],\n",
    "    \"opset_version\":      [17],\n",
    "    \"do_constant_folding\":[True, False],\n",
    "    \"dynamic_axes\": [\n",
    "        {'input_signal': {0: 'batch'}, 'input_signal_length': {0: 'batch'}, 'logits': {0: 'batch'}},\n",
    "        None\n",
    "    ]\n",
    "}\n",
    "param_grid = list(product(*param_space.values()))\n",
    "print(f\"Total combinations: {len(param_grid)}\")\n",
    "os.makedirs(\"onnx_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7e979e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAIL] onnx_models/asr_model_0.onnx → STFT does not currently support complex types  [Caused by the value '826 defined in (%826 : Float(*, *, strides=[159904, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%815, %825), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py:729:0\n",
      ")' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Reshape'.] \n",
      "    (node defined in d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py(729): stft\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\preprocessing\\features.py(384): stft\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\preprocessing\\features.py(437): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\utils\\_contextlib.py(116): decorate_context\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\modules\\audio_preprocessing.py(301): get_features\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\modules\\audio_preprocessing.py(101): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\utils\\_contextlib.py(116): decorate_context\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py(1081): wrapped_call\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\models\\ctc_models.py(528): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py(1081): wrapped_call\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_32696\\1959213485.py(8): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(129): wrapper\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(138): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(1501): _get_trace_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(878): _trace_and_get_graph_from_model\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(971): _create_jit_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(1087): _model_to_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(1467): _export\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(529): export\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\__init__.py(396): export\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_32696\\845553637.py(6): <module>\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3670): run_code\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3610): run_ast_nodes\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3365): run_cell_async\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\async_helpers.py(128): _pseudo_sync_runner\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3153): _run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3098): run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\zmqshell.py(549): run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\ipkernel.py(449): do_execute\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(778): execute_request\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\ipkernel.py(362): execute_request\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(437): dispatch_shell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(534): process_one\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(545): dispatch_queue\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\events.py(84): _run\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\base_events.py(1936): _run_once\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\base_events.py(608): run_forever\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\tornado\\platform\\asyncio.py(211): start\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelapp.py(739): start\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\traitlets\\config\\application.py(1075): launch_instance\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel_launcher.py(18): <module>\n",
      "<frozen runpy>(88): _run_code\n",
      "<frozen runpy>(198): _run_module_as_main\n",
      ")\n",
      "\n",
      "    Inputs:\n",
      "        #0: 815 defined in (%815 : Float(*, *, *, strides=[159904, 159904, 1], requires_grad=0, device=cpu) = onnx::Pad[mode=\"reflect\"](%788, %814), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\functional.py:5209:0\n",
      "    )  (type 'Tensor')\n",
      "        #1: 825 defined in (%825 : int[] = prim::ListConstruct(%819, %824), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer\n",
      "    )  (type 'List[int]')\n",
      "    Outputs:\n",
      "        #0: 826 defined in (%826 : Float(*, *, strides=[159904, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%815, %825), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py:729:0\n",
      "    )  (type 'Tensor')\n",
      "[FAIL] onnx_models/asr_model_1.onnx → STFT does not currently support complex types  [Caused by the value '828 defined in (%828 : Float(*, *, strides=[159904, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%817, %827), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py:729:0\n",
      ")' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Reshape'.] \n",
      "    (node defined in d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py(729): stft\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\preprocessing\\features.py(384): stft\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\preprocessing\\features.py(437): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\utils\\_contextlib.py(116): decorate_context\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\modules\\audio_preprocessing.py(301): get_features\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\modules\\audio_preprocessing.py(101): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\utils\\_contextlib.py(116): decorate_context\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py(1081): wrapped_call\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\models\\ctc_models.py(528): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py(1081): wrapped_call\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_32696\\1959213485.py(8): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(129): wrapper\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(138): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(1501): _get_trace_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(878): _trace_and_get_graph_from_model\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(971): _create_jit_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(1087): _model_to_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(1467): _export\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(529): export\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\__init__.py(396): export\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_32696\\845553637.py(6): <module>\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3670): run_code\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3610): run_ast_nodes\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3365): run_cell_async\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\async_helpers.py(128): _pseudo_sync_runner\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3153): _run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3098): run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\zmqshell.py(549): run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\ipkernel.py(449): do_execute\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(778): execute_request\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\ipkernel.py(362): execute_request\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(437): dispatch_shell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(534): process_one\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(545): dispatch_queue\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\events.py(84): _run\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\base_events.py(1936): _run_once\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\base_events.py(608): run_forever\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\tornado\\platform\\asyncio.py(211): start\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelapp.py(739): start\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\traitlets\\config\\application.py(1075): launch_instance\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel_launcher.py(18): <module>\n",
      "<frozen runpy>(88): _run_code\n",
      "<frozen runpy>(198): _run_module_as_main\n",
      ")\n",
      "\n",
      "    Inputs:\n",
      "        #0: 817 defined in (%817 : Float(*, *, *, strides=[159904, 159904, 1], requires_grad=0, device=cpu) = onnx::Pad[mode=\"reflect\"](%790, %816), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\functional.py:5209:0\n",
      "    )  (type 'Tensor')\n",
      "        #1: 827 defined in (%827 : int[] = prim::ListConstruct(%821, %826), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer\n",
      "    )  (type 'List[int]')\n",
      "    Outputs:\n",
      "        #0: 828 defined in (%828 : Float(*, *, strides=[159904, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%817, %827), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py:729:0\n",
      "    )  (type 'Tensor')\n",
      "[FAIL] onnx_models/asr_model_2.onnx → STFT does not currently support complex types  [Caused by the value '826 defined in (%826 : Float(*, *, strides=[159904, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%815, %825), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py:729:0\n",
      ")' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Reshape'.] \n",
      "    (node defined in d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py(729): stft\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\preprocessing\\features.py(384): stft\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\preprocessing\\features.py(437): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\utils\\_contextlib.py(116): decorate_context\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\modules\\audio_preprocessing.py(301): get_features\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\modules\\audio_preprocessing.py(101): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\utils\\_contextlib.py(116): decorate_context\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py(1081): wrapped_call\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\models\\ctc_models.py(528): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py(1081): wrapped_call\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_32696\\1959213485.py(8): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(129): wrapper\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(138): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(1501): _get_trace_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(878): _trace_and_get_graph_from_model\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(971): _create_jit_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(1087): _model_to_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(1467): _export\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(529): export\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\__init__.py(396): export\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_32696\\845553637.py(6): <module>\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3670): run_code\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3610): run_ast_nodes\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3365): run_cell_async\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\async_helpers.py(128): _pseudo_sync_runner\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3153): _run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3098): run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\zmqshell.py(549): run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\ipkernel.py(449): do_execute\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(778): execute_request\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\ipkernel.py(362): execute_request\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(437): dispatch_shell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(534): process_one\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(545): dispatch_queue\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\events.py(84): _run\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\base_events.py(1936): _run_once\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\base_events.py(608): run_forever\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\tornado\\platform\\asyncio.py(211): start\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelapp.py(739): start\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\traitlets\\config\\application.py(1075): launch_instance\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel_launcher.py(18): <module>\n",
      "<frozen runpy>(88): _run_code\n",
      "<frozen runpy>(198): _run_module_as_main\n",
      ")\n",
      "\n",
      "    Inputs:\n",
      "        #0: 815 defined in (%815 : Float(*, *, *, strides=[159904, 159904, 1], requires_grad=0, device=cpu) = onnx::Pad[mode=\"reflect\"](%788, %814), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\functional.py:5209:0\n",
      "    )  (type 'Tensor')\n",
      "        #1: 825 defined in (%825 : int[] = prim::ListConstruct(%819, %824), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer\n",
      "    )  (type 'List[int]')\n",
      "    Outputs:\n",
      "        #0: 826 defined in (%826 : Float(*, *, strides=[159904, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%815, %825), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py:729:0\n",
      "    )  (type 'Tensor')\n",
      "[FAIL] onnx_models/asr_model_3.onnx → STFT does not currently support complex types  [Caused by the value '828 defined in (%828 : Float(*, *, strides=[159904, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%817, %827), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py:729:0\n",
      ")' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Reshape'.] \n",
      "    (node defined in d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py(729): stft\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\preprocessing\\features.py(384): stft\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\parts\\preprocessing\\features.py(437): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\utils\\_contextlib.py(116): decorate_context\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\modules\\audio_preprocessing.py(301): get_features\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\modules\\audio_preprocessing.py(101): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\utils\\_contextlib.py(116): decorate_context\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py(1081): wrapped_call\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\collections\\asr\\models\\ctc_models.py(528): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py(1081): wrapped_call\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_32696\\1959213485.py(8): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1741): _slow_forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(129): wrapper\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(138): forward\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1762): _call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py(1751): _wrapped_call_impl\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py(1501): _get_trace_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(878): _trace_and_get_graph_from_model\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(971): _create_jit_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(1087): _model_to_graph\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(1467): _export\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\utils.py(529): export\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\onnx\\__init__.py(396): export\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_32696\\845553637.py(6): <module>\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3670): run_code\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3610): run_ast_nodes\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3365): run_cell_async\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\async_helpers.py(128): _pseudo_sync_runner\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3153): _run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3098): run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\zmqshell.py(549): run_cell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\ipkernel.py(449): do_execute\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(778): execute_request\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\ipkernel.py(362): execute_request\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(437): dispatch_shell\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(534): process_one\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelbase.py(545): dispatch_queue\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\events.py(84): _run\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\base_events.py(1936): _run_once\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\asyncio\\base_events.py(608): run_forever\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\tornado\\platform\\asyncio.py(211): start\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel\\kernelapp.py(739): start\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\traitlets\\config\\application.py(1075): launch_instance\n",
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\ipykernel_launcher.py(18): <module>\n",
      "<frozen runpy>(88): _run_code\n",
      "<frozen runpy>(198): _run_module_as_main\n",
      ")\n",
      "\n",
      "    Inputs:\n",
      "        #0: 817 defined in (%817 : Float(*, *, *, strides=[159904, 159904, 1], requires_grad=0, device=cpu) = onnx::Pad[mode=\"reflect\"](%790, %816), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\functional.py:5209:0\n",
      "    )  (type 'Tensor')\n",
      "        #1: 827 defined in (%827 : int[] = prim::ListConstruct(%821, %826), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer\n",
      "    )  (type 'List[int]')\n",
      "    Outputs:\n",
      "        #0: 828 defined in (%828 : Float(*, *, strides=[159904, 1], requires_grad=0, device=cpu) = onnx::Reshape[allowzero=0](%817, %827), scope: __main__.ASRWrapper::/nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::model/nemo.collections.asr.modules.audio_preprocessing.AudioToMelSpectrogramPreprocessor::preprocessor/nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures::featurizer # d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\functional.py:729:0\n",
      "    )  (type 'Tensor')\n"
     ]
    }
   ],
   "source": [
    "# --- Export loop ---\n",
    "exported_models = []\n",
    "for idx, (export_params, opset, const_fold, dyn_axes) in enumerate(param_grid):\n",
    "    out_path = f\"onnx_models/asr_model_{idx}.onnx\"\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            wrapper,\n",
    "            args=(audio_input, length),\n",
    "            f=out_path,\n",
    "            input_names=['input_signal', 'input_signal_length'],\n",
    "            output_names=['logits'],\n",
    "            export_params=export_params,\n",
    "            opset_version=opset,\n",
    "            do_constant_folding=const_fold,\n",
    "            dynamic_axes=dyn_axes,\n",
    "            verbose=False\n",
    "        )\n",
    "        exported_models.append((out_path, export_params, opset, const_fold, dyn_axes))\n",
    "        print(f\"[OK] {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] {out_path} → {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Benchmarking ---\n",
    "def to_numpy(t: torch.Tensor):\n",
    "    return t.detach().cpu().numpy()\n",
    "\n",
    "# 1. PyTorch timing\n",
    "torch_times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(5):\n",
    "        t0 = time.time()\n",
    "        _ = wrapper(audio_input, length)\n",
    "        torch_times.append(time.time() - t0)\n",
    "torch_avg = sum(torch_times) / len(torch_times)\n",
    "print(f\"PyTorch avg inference: {torch_avg:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844701b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAIL] onnx_models/asr_model_0.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_1.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_2.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_3.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_4.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_5.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_6.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_7.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_8.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_9.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_10.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n",
      "[FAIL] onnx_models/asr_model_11.onnx → Input argument audio_signal has no corresponding input_type match. Existing input_types = dict_keys(['input_signal', 'input_signal_length', 'processed_signal', 'processed_signal_length', 'sample_id'])\n"
     ]
    }
   ],
   "source": [
    "# 2. ONNX timing\n",
    "onnx_results = []\n",
    "for path, *_ in exported_models:\n",
    "    try:\n",
    "        sess = onnxruntime.InferenceSession(path, providers=[\"CPUExecutionProvider\"])\n",
    "        inp_names = [i.name for i in sess.get_inputs()]\n",
    "        input_dict = {\n",
    "            inp_names[0]: to_numpy(audio_input),\n",
    "            inp_names[1]: to_numpy(length)\n",
    "        }\n",
    "        times = []\n",
    "        for _ in range(5):\n",
    "            t0 = time.time()\n",
    "            _ = sess.run(None, input_dict)\n",
    "            times.append(time.time() - t0)\n",
    "        onnx_results.append((path, sum(times)/len(times)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Results\n",
    "onnx_results.sort(key=lambda x: x[1])\n",
    "for p, t in onnx_results:\n",
    "    print(f\"{os.path.basename(p)}: {t:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Plot\n",
    "labels = [\"PyTorch\"] + [os.path.basename(p) for p,_ in onnx_results]\n",
    "times  = [torch_avg] + [t for _,t in onnx_results]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(labels, times)\n",
    "plt.xlabel(\"Avg Inference Time (sec)\")\n",
    "plt.title(\"PyTorch vs ONNX Variants\")\n",
    "plt.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Best model\n",
    "best_model = onnx_results[0][0]\n",
    "print(f\"✅ Best ONNX model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03c8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10802973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-24 17:54:57 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-24 17:54:57 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /code/03_manifest/ulca-train-v3.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 16.7\n",
      "    min_duration: 0.1\n",
      "    \n",
      "[NeMo W 2025-05-24 17:54:57 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2025-05-24 17:54:57 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-24 17:54:57 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2025-05-24 17:54:57 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from d:\\IIT BBS\\Job Resources\\mvaakAI\\asr_app\\model\\stt_hi_conformer_ctc_medium.nemo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncDecCTCModelBPE(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): ConformerEncoder(\n",
       "    (pre_encode): ConvSubsampling(\n",
       "      (out): Linear(in_features=5120, out_features=256, bias=True)\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_enc): RelPositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x ConformerLayer(\n",
       "        (norm_feed_forward1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward1): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): ConformerConvolution(\n",
       "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (depthwise_conv): CausalConv1D(256, 256, kernel_size=(31,), stride=(1,), groups=256)\n",
       "          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): Swish()\n",
       "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (norm_self_att): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): RelPositionMultiHeadAttention(\n",
       "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (norm_feed_forward2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward2): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvASRDecoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Conv1d(256, 129, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (loss): CTCLoss()\n",
       "  (spec_augmentation): SpectrogramAugmentation(\n",
       "    (spec_augment): SpecAugment()\n",
       "  )\n",
       "  (wer): WER()\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from nemo.collections.asr.models import EncDecCTCModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "asr_model = EncDecCTCModel.restore_from(\"model/stt_hi_conformer_ctc_medium.nemo\")\n",
    "asr_model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7fe9b097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio loaded: test/test1.wav\n",
      "Original SR: 16000, Resampled SR: 16000\n",
      "Input Tensor Shape: torch.Size([1, 159392])\n",
      "Duration (s): 9.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0001,  0.0000, -0.0004]]),\n",
       " tensor([159392]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_audio(filepath, target_sr=16000, expected_input_shape=(1, -1)):\n",
    "    try:\n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(filepath)  # waveform shape: (channels, samples)\n",
    "\n",
    "        # Check if stereo and convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Normalize audio amplitude to [-1, 1]\n",
    "        waveform = waveform / waveform.abs().max()\n",
    "\n",
    "        # Confirm shape\n",
    "        waveform = waveform.squeeze(0)  # now shape is [samples]\n",
    "        input_tensor = waveform.unsqueeze(0)  # shape [1, time] for batch dimension\n",
    "\n",
    "        print(f\"Audio loaded: {filepath}\")\n",
    "        print(f\"Original SR: {sr}, Resampled SR: {target_sr}\")\n",
    "        print(f\"Input Tensor Shape: {input_tensor.shape}\")\n",
    "        print(f\"Duration (s): {input_tensor.shape[-1] / target_sr:.2f}\")\n",
    "        return input_tensor, torch.tensor([input_tensor.shape[-1]], dtype=torch.int64)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {filepath}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "load_and_preprocess_audio(\"test/test1.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f08a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the waveform\n",
    "waveform, sample_rate = torchaudio.load(\"test/test1.wav\")  # shape: [channels, time]\n",
    "\n",
    "# If stereo, convert to mono\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# Remove channel dimension to get [time], then unsqueeze batch to get [1, time]\n",
    "input_signal = waveform.squeeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "# Get input lengths\n",
    "input_length = torch.tensor([input_signal.shape[1]], dtype=torch.int64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "441d95c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input shape mismatch occured for input_signal in module EncDecCTCModelBPE : \nInput shape expected = (batch, time) | \nInput shape found : torch.Size([1, 2, 159392])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m wrapper = TorchScriptWrapper(asr_model).eval().to(device)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     traced_model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     traced_model.save(\u001b[33m\"\u001b[39m\u001b[33mnemo_conformer_traced.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTorchScript model saved as nemo_conformer_traced.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py:1002\u001b[39m, in \u001b[36mtrace\u001b[39m\u001b[34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[39m\n\u001b[32m    989\u001b[39m     warnings.warn(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`optimize` is deprecated and has no effect. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse `with torch.jit.optimized_execution()` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    992\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    993\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    994\u001b[39m     )\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    997\u001b[39m     check_if_torch_exportable,\n\u001b[32m    998\u001b[39m     log_torch_jit_trace_exportability,\n\u001b[32m    999\u001b[39m     log_torchscript_usage,\n\u001b[32m   1000\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m traced_func = \u001b[43m_trace_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m log_torchscript_usage(\u001b[33m\"\u001b[39m\u001b[33mtrace\u001b[39m\u001b[33m\"\u001b[39m, model_id=_get_model_id(traced_func))\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_if_torch_exportable():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py:696\u001b[39m, in \u001b[36m_trace_impl\u001b[39m\u001b[34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[39m\n\u001b[32m    694\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    695\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mexample_kwarg_inputs should be a dict\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    710\u001b[39m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[33m\"\u001b[39m\u001b[33m__self__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    711\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func.\u001b[34m__self__\u001b[39m, torch.nn.Module)\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    713\u001b[39m ):\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\jit\\_trace.py:1279\u001b[39m, in \u001b[36mtrace_module\u001b[39m\u001b[34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[39m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     example_inputs = make_tuple(example_inputs)\n\u001b[32m-> \u001b[39m\u001b[32m1279\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_c\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1290\u001b[39m check_trace_method = module._c._get_method(method_name)\n\u001b[32m   1292\u001b[39m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1741\u001b[39m, in \u001b[36mModule._slow_forward\u001b[39m\u001b[34m(self, *input, **kwargs)\u001b[39m\n\u001b[32m   1739\u001b[39m         recording_scopes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1740\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1741\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1743\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mTorchScriptWrapper.forward\u001b[39m\u001b[34m(self, input_signal, input_signal_length)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_signal, input_signal_length):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_signal\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_signal_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_signal_length\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1741\u001b[39m, in \u001b[36mModule._slow_forward\u001b[39m\u001b[34m(self, *input, **kwargs)\u001b[39m\n\u001b[32m   1739\u001b[39m         recording_scopes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1740\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1741\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1743\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py:1078\u001b[39m, in \u001b[36mtypecheck.wrapped_call\u001b[39m\u001b[34m(self, wrapped, instance, args, kwargs)\u001b[39m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arguments must be passed by kwargs only for typed methods\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1077\u001b[39m \u001b[38;5;66;03m# Perform rudimentary input checks here\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_input_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_collections\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_collections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[38;5;66;03m# Call the method - this can be forward, or any other callable method\u001b[39;00m\n\u001b[32m   1081\u001b[39m outputs = wrapped(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\nemo\\core\\classes\\common.py:246\u001b[39m, in \u001b[36mTyping._validate_input_types\u001b[39m\u001b[34m(self, input_types, ignore_collections, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m     name = key\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m type_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value_shape) != \u001b[38;5;28mlen\u001b[39m(type_shape):\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    247\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput shape mismatch occured for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput shape expected = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata.base_types[key].axes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    249\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput shape found : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m         )\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Perform recursive neural type check for homogeneous elements\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[31mTypeError\u001b[39m: Input shape mismatch occured for input_signal in module EncDecCTCModelBPE : \nInput shape expected = (batch, time) | \nInput shape found : torch.Size([1, 2, 159392])"
     ]
    }
   ],
   "source": [
    "# TorchScript wrapper\n",
    "class TorchScriptWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_signal, input_signal_length):\n",
    "        return self.model(input_signal=input_signal, input_signal_length=input_signal_length)\n",
    "\n",
    "# Create wrapper and trace\n",
    "wrapper = TorchScriptWrapper(asr_model).eval().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    traced_model = torch.jit.trace(wrapper, (sample_input, input_length))\n",
    "    traced_model.save(\"nemo_conformer_traced.pt\")\n",
    "\n",
    "print(\"TorchScript model saved as nemo_conformer_traced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120046d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "search_space = {\n",
    "    'batch_size': [1, 2, 4],\n",
    "    'input_length_sec': [5, 10, 15],\n",
    "    'precision': ['float32', 'float16']\n",
    "}\n",
    "\n",
    "# Create all combinations\n",
    "configurations = list(itertools.product(\n",
    "    search_space['batch_size'],\n",
    "    search_space['input_length_sec'],\n",
    "    search_space['precision']\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_dummy_input(batch_size, length_sec, precision, sample_rate=16000):\n",
    "    length = int(sample_rate * length_sec)\n",
    "    dtype = torch.float16 if precision == 'float16' else torch.float32\n",
    "    dummy_audio = torch.randn(batch_size, length, dtype=dtype).cuda()\n",
    "    dummy_length = torch.full((batch_size,), length, dtype=torch.int32).cuda()\n",
    "    return dummy_audio, dummy_length\n",
    "\n",
    "results = []\n",
    "\n",
    "for batch_size, input_sec, precision in configurations:\n",
    "    dummy_input, dummy_len = generate_dummy_input(batch_size, input_sec, precision)\n",
    "\n",
    "    # Convert model to the right precision\n",
    "    if precision == 'float16':\n",
    "        model = scripted_model.half()\n",
    "    else:\n",
    "        model = scripted_model.float()\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(5):\n",
    "        _ = model(dummy_input, dummy_len)\n",
    "\n",
    "    # Benchmark\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input, dummy_len)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    avg_time = (end - start) / 10\n",
    "    results.append({\n",
    "        'batch_size': batch_size,\n",
    "        'input_sec': input_sec,\n",
    "        'precision': precision,\n",
    "        'inference_time': avg_time\n",
    "    })\n",
    "\n",
    "    print(f\"Batch {batch_size}, Input {input_sec}s, {precision}: {avg_time:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f445e2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2441e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8980d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f1de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b3dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2f14b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2025-05-24 20:52:11 nemo_logging:405] d:\\Anaconda\\envs\\mvaakai\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from nemo.collections.asr.models import EncDecCTCModel\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import onnxruntime as ort\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b18550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Function: Preprocess audio ===\n",
    "def preprocess_audio(file_path, target_sr=16000):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)  # (channels, time)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        print(\"Stereo audio detected. Converting to mono.\")\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sample_rate != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Normalize\n",
    "    waveform = waveform / waveform.abs().max()\n",
    "\n",
    "    # (batch, time)\n",
    "    input_tensor = waveform.squeeze(0).unsqueeze(0)\n",
    "    input_length = torch.tensor([input_tensor.shape[1]], dtype=torch.int64)\n",
    "\n",
    "    return input_tensor.to(device), input_length.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7f1c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stereo audio detected. Converting to mono.\n"
     ]
    }
   ],
   "source": [
    "# === Preprocess a real audio file ===\n",
    "input_signal, input_length = preprocess_audio(\"test/test1.wav\")\n",
    "input_signal = input_signal.to(device)\n",
    "input_length = input_length.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564b8394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-24 20:52:52 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-24 20:52:52 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /code/03_manifest/ulca-train-v3.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 16.7\n",
      "    min_duration: 0.1\n",
      "    \n",
      "[NeMo W 2025-05-24 20:52:52 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n",
      "[NeMo W 2025-05-24 20:52:52 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: /code/manifest/ulca-eval.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-24 20:52:52 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2025-05-24 20:52:53 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from d:\\IIT BBS\\Job Resources\\mvaakAI\\asr_app\\model\\stt_hi_conformer_ctc_medium.nemo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncDecCTCModelBPE(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): ConformerEncoder(\n",
       "    (pre_encode): ConvSubsampling(\n",
       "      (out): Linear(in_features=5120, out_features=256, bias=True)\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_enc): RelPositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x ConformerLayer(\n",
       "        (norm_feed_forward1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward1): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): ConformerConvolution(\n",
       "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "          (depthwise_conv): CausalConv1D(256, 256, kernel_size=(31,), stride=(1,), groups=256)\n",
       "          (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): Swish()\n",
       "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (norm_self_att): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): RelPositionMultiHeadAttention(\n",
       "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (norm_feed_forward2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward2): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm_out): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ConvASRDecoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Conv1d(256, 129, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (loss): CTCLoss()\n",
       "  (spec_augmentation): SpectrogramAugmentation(\n",
       "    (spec_augment): SpecAugment()\n",
       "  )\n",
       "  (wer): WER()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load model ===\n",
    "asr_model = EncDecCTCModel.restore_from(\"model/stt_hi_conformer_ctc_medium.nemo\")\n",
    "asr_model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8784de1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TorchScript model saved as nemo_conformer_traced.pt\n"
     ]
    }
   ],
   "source": [
    "# === TorchScript wrapper ===\n",
    "class TorchScriptWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_signal, input_signal_length):\n",
    "        return self.model(input_signal=input_signal, input_signal_length=input_signal_length)\n",
    "\n",
    "wrapper = TorchScriptWrapper(asr_model).eval().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    traced_model = torch.jit.trace(wrapper, (input_signal, input_length))\n",
    "    traced_model.save(\"model/nemo_conformer_traced.pt\")\n",
    "\n",
    "print(\"✅ TorchScript model saved as nemo_conformer_traced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e577153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Batch 1, 5s, float32: 0.1452s\n",
      "⏱️ Batch 1, 10s, float32: 0.2122s\n",
      "⏱️ Batch 1, 15s, float32: 0.2817s\n",
      "⏱️ Batch 2, 5s, float32: 0.1890s\n",
      "⏱️ Batch 2, 10s, float32: 0.3447s\n",
      "⏱️ Batch 2, 15s, float32: 0.5540s\n",
      "⏱️ Batch 4, 5s, float32: 0.3999s\n",
      "⏱️ Batch 4, 10s, float32: 0.8377s\n",
      "⏱️ Batch 4, 15s, float32: 1.3527s\n"
     ]
    }
   ],
   "source": [
    "# === Inference benchmarking ===\n",
    "\n",
    "scripted_model = torch.jit.load(\"model/nemo_conformer_traced.pt\").to(device)\n",
    "\n",
    "search_space = {\n",
    "    'batch_size': [1, 2, 4],\n",
    "    'input_length_sec': [5, 10, 15],\n",
    "    'precision': ['float32']  # CPU: no float16!\n",
    "}\n",
    "configurations = list(itertools.product(\n",
    "    search_space['batch_size'],\n",
    "    search_space['input_length_sec'],\n",
    "    search_space['precision']\n",
    "))\n",
    "\n",
    "def generate_dummy_input(batch_size, length_sec, precision, sample_rate=16000):\n",
    "    length = int(sample_rate * length_sec)\n",
    "    dtype = torch.float32  # Only float32 on CPU\n",
    "    dummy_audio = torch.randn(batch_size, length, dtype=dtype).to(device)\n",
    "    dummy_length = torch.full((batch_size,), length, dtype=torch.int32).to(device)\n",
    "    return dummy_audio, dummy_length\n",
    "\n",
    "results = []\n",
    "\n",
    "for batch_size, input_sec, precision in configurations:\n",
    "    dummy_input, dummy_len = generate_dummy_input(batch_size, input_sec, precision)\n",
    "\n",
    "    model = scripted_model.float()\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(2):\n",
    "        _ = model(dummy_input, dummy_len)\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(5):\n",
    "        _ = model(dummy_input, dummy_len)\n",
    "    end = time.time()\n",
    "\n",
    "    avg_time = (end - start) / 5\n",
    "    results.append({\n",
    "        'batch_size': batch_size,\n",
    "        'input_sec': input_sec,\n",
    "        'precision': precision,\n",
    "        'inference_time': avg_time\n",
    "    })\n",
    "\n",
    "    print(f\"⏱️ Batch {batch_size}, {input_sec}s, {precision}: {avg_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f6cee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAIjCAYAAAB/OVoZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXIJJREFUeJzt3QmcjXX///HPmM06luxuQrZkjbhRSZZJUlrupMWQVKJEFGVJRFFoIVuSyhKVSrIkQpQM2m77krJTGUxmmDn/x+f7+1/nPufMGeZwlmvOeT0fj9M411znXN9zXddcnff13aIcDodDAAAAAABAyOUJdQEAAAAAAMD/IaQDAAAAAGAThHQAAAAAAGyCkA4AAAAAgE0Q0gEAAAAAsAlCOgAAAAAANkFIBwAAAADAJgjpAAAAAADYBCEdAAAAAACbIKQDgA9OnTolDz30kJQuXVqioqLkySefDHWRbG/lypVmX+lPIBjn2vz58y/pfUaPHi01atSQzMxMyc0WL14s9erVk7x585r98vfff0uXLl2kYsWKoS5arrN3716zD2fMmOHza++55x65++67A1IuAOGJkA4gougXLP2itWHDhot6/ciRI8179OjRQ9577z154IEHJBLpF33djxd66HqRSD97r169xC5mzZol48ePz/H6GuJuueUWsStfP48vUlJS5OWXX5ZnnnlG8uRx/5p05swZGTdunDRu3FgKFy5swm+1atXMsd6+fbtzveeff97t7yB//vxSs2ZNGTRokHl/z/WOHTvmtSy1atWSG2644aI+x/Hjx00wzJcvn0yYMMFcrwoUKCDBMHHiRK9h9sCBA3L//fdL9erVpVChQlKkSBFp1KiRvPvuu+JwONzW/fjjj6Vjx45SuXJls//0NU899ZS50ZDb6Ln00UcfyY8//hjqogDIJWJCXQAAyE2+/vpr+fe//y1Dhw6VSPbII49Iq1atnM/37NkjQ4YMkYcffliuu+465/IrrrjCBJp//vlH4uLiQlRaaKj95ZdfwqblRyA/z/Tp0+XcuXPSqVMnt+UapG+66SZJTk42NzDuvfdeKViwoGzbtk3mzJkjU6ZMkfT0dLfXvPXWW2YdbYGzdOlSefHFF8015NtvvzXhPJB++OEHOXnypAwfPtztbzVYIb148eJZbtLpPvzjjz/krrvukgoVKsjZs2dl2bJlZj3dj3oT1KLXkrJly5pQr+v+/PPP8uabb8qiRYtk48aN5uZDblG/fn1p2LChvPrqqzJz5sxQFwdALkBIBwAfHDlyxNSI+Ys2p9Uv9lojl5s0adLEPCzaMkFDui7TL9WectvnQ+R655135NZbb81yzmqQ3LRpk2lKf+edd7r9ToPwc889l+W9NIxqWFWPPvqoeZ3WEH/33Xdufz+BulYpra22izp16mTp9qKtENq3by+vv/662Y/R0dFmue5nz1YEDRo0kKSkJPnggw9Mt6PcRFs16M1dvYGhN24A4Hxo7g4g4umXb/3StH//funQoYP5d4kSJaRfv36SkZHh1tdVa4y/+OILZzNW7aeo0tLSzBewKlWqSHx8vJQvX16efvpps9xbM2j9knnVVVeZdbXfqNLtP/jgg1KqVCmzXH+vtXqurHJ8+OGHplbuX//6lwkTLVu2lJ07d2b5bN9//73cfPPNUrRoUdPUVb8kv/baa27rbN261YSJYsWKmffSGp/PPvssoH3S9cu3NuX96aefpHnz5qY5q+47qy/xN998Y2rgtbZMm7l+9dVXWd43J/vLG91uixYtvN4wKVeunNkXFq0h1WCgTXMTEhKkdu3aWfafL/sgJ8fN2jdaY9u0aVOzDypVqiSTJk3y2nXDOgez29/6fnrO/vbbb87z1l99kt9//32zf7SMev5o39vff//d6+f573//a/a7Hmvdz9rv25OWUQOynqslS5aUPn36yJIlS3z+PHosc/L34Un/vvWc9Kx51r8j3Wa3bt2yBHSl598rr7xywfe/8cYbndsJJN1HGmbVNddcc8GuJ6dPnzZNyfW6pZ9F/+b083g2QdcbGPoZ9NjoenrDUlsLuNJj8euvv5q/Yev4XKjJvr4mNTXVrSWCt9fcfvvt5ueWLVtytB80EFvXWa2V79mzZ5bm8r6cn577Qj+b3rjxpC0C9GaDXqMsrVu3NvtZWw4AwIVQkw4AIiaMJyYmmmCoX041FGrTRG2urf3Pr7zyStOnU0ODfvHXL7RKw7wGAg0Wa9asMU00dV1tmql9V7Wf6oIFC9y2pc1dNaxpWNdaNv2CevjwYdOM3grx+r5ffvmlCQXah9WzWe9LL71k+svqjYQTJ06YL5T33XefCRMW/TKozXLLlCkjvXv3NoPd6ZfbhQsXmudKv0w3a9bMfCkdMGCACUdaNr1ZoX0orS/FgfDXX3+Z8mmw+89//mO+7Ou/9QaGfl6tedQmxWPGjDHBWcOfhmXl6/5ypf1ctS/woUOHzD6x6PHTPrNaBmv/aZNnDXjaR1np/tOmytb+81VOjpu1b/Tmita+aRn0mOh5qF0G9MaEL7SGV7elzYz1nFT+qMnTEDx48GBTRq3VPHr0qLzxxhty/fXXm+DiWoOrn0ebit9xxx1mfb0Zo/109aZH27ZtzToaYDQAHjx40Hm+arP2FStW+Px5crqfPa1du9b8vPrqq92WWzetLnUMil27dpmfl112mQSS7iMN2toE/4UXXjA3efRa5o0Gcb1+6X7Wvx8daE5vjPTv39+ETGsfK/0b1dCr68fExMjnn38ujz32mLkGagBWOlbA448/bo6J1bpAb6S50u4very1G4CGeQ282rLgQk3Y9W9WWa0Tzkf/xocNG2ZuuOjfjjan1/JrNwD9G46NjfXp/PSk1yT9zHq90ubsrnSZhn+9rlr0hoZ+Pt12IK+rAMKEAwAiyDvvvKNVQ44ffvjBuSwpKckse+GFF9zWrV+/vqNBgwZuyy6//HJHu3bt3Ja99957jjx58jhWr17ttnzSpEnmfb/99lvnMn2u6/76669u63br1s1RpkwZx7Fjx9yW33PPPY7ChQs7UlNTzfMVK1aY97jyyisdaWlpzvVee+01s/znn382z8+dO+eoVKmSKe9ff/3l9p6ZmZnOf7ds2dJRu3Ztx5kzZ9x+37RpU0fVqlUdOaX7U7ev+9eTVWb9aWnevLlZNmvWLOeyrVu3OvfPd99951y+ZMmSLO+d0/3lzbZt28z7vfHGG27LH3vsMUfBggWdr+3du7cjISHB7Etf6fv37Nkzyz640HFz3Tevvvqqc5m+pl69eo6SJUs60tPT3c7lPXv2XHB/6zmr50JOeTvPXe3du9cRHR3tePHFF92W6+eIiYlxW259npkzZ7p9ntKlSzvuvPNO5zL9vLreggULnMv++ecfR40aNXL8eXzZz94MGjTIrHfy5Em35bfffrtZ7vm3lJ2hQ4ea9fVcO3r0qDlGkydPdsTHxztKlSrlOH36tNt6uo43V111ldl//rrWWdc7132n+1vXGzFihNt6d911lyMqKsqxc+dO5zJvf1eJiYmOypUr+1TuUaNGmW1aD70O7du374KfSf/u9bzbvn37edc7cuSIIy4uztGmTRtHRkaGc/mbb75ptjd9+nSfz089hp7XoU6dOjnKli3rto2NGzdmey2sVq2ao23bthf8nABAc3cA+P+05taVDoC2e/fuC75u3rx5pvZcp2zSgZGsh9W01bMmUJt3u/Zr10yntdbaL1P/7foeWruvNYE6UJKrrl27ug3EZg3WZpVXazK1Sa3WKHv2SbUGrPrzzz9Nrb7WHOkAU9Y2dVRo3e6OHTvcmmv6m9a0WbXWSmv+tKy6L7VFg8X6t/XZLmZ/udLRuLW2cO7cuW4tKbT2TN/Tqs3Tsvi7eeqFjptFayl1cD6Lvkafaz9jbQYfatqvWmtP9dxx3f9a+121atUs57wea9exCvTz6Kjerp9bu31ozaPW0lq0qXr37t0Dtp896bmv+96zZt4akd1qyZFTek5rKw+tydbjp106tNm8Nqm2Cx2ITZtmP/HEE27LtbWQ/n1pCxWLa023/p3pMdfrme5XfZ5T2jpE/660pYS2lrFq189H13377bdNufQcOx9tCaVN5/X65zpCv55L2m1Fj4Gv56c3nTt3Nq1vXM93rUXX/eStW4R2O8puJH8AcEVzdwD4/2FAv0x7fqHSZpAXomFWm0F7vt5zACeLfmF3pc2EtZ+kNk3VR07eQ0c79iyrssprNavVvpbZ0T66+iVcmyzrI7vtujbZ9CftNuA5wrVOa6X9Yj2XuX62i9lf3pq8P/vss+YmhH4+7e+sr9HlFm3Gq83MtbmrrtOmTRsTSrVZ7MW60HGzaP9Zz+my9OaC0j7o2tQ/lPSc13Mnu7Dk2pQ4u2Otn137f1u0j7k2yfZcT4NtoPZzTmmwU3ozy5eB2PRmkr5W94fug+yanJ9PoEeB1/2u55vnDQi9WWb93qJNtXXsjXXr1pk+5K40pFt/qxdy+eWXm4cV2LWbkDZL1ybp3pq8r1692jTF15tw2s0iJ5/JukniSsO3Tunm+plyen56o/3MtTuRBnPtFqM3rmbPni233Xab1xs6+jcT6OMJIDwQ0gFAxDmi8MXQL2bad3Hs2LFef+8ZOj2/hOrrldbkWIM9edIB33JSXs+Bni5UbqX9dvXLrzcXE5ByKrvPcKHPdjH7y5OG8YEDB5pWEFrbpmFcA4ZrANfBsTZv3mz652ptoj6076zWnum8zhfDH8fNkt2XfWuww0DSY6Db133i7TN51kT783PnxMVuT/uK6/RrGsZdQ5a2klE61oTrFIMXov3zz9d/2hpBPrtaZA3CdpkZQW/8aRDVfaHXOr2uaejVmnjtt279XV4M7d89depUWbVqVZZrkc4trq0r9IajtnbRlg52OV/0ddoSQMuug9TpTQxrLnhv9CbRhVoBAIAipAPAJdLaMf0iqV9gL6aWRGvgNRBouPLXfMZWjZ3OJZ3de2qNktJavmDPo3wp/LG/tDWDNmfVJu868Jw239bB8nQUaFcaQrQJvD40hGjt+uTJk03Lg0DewNAv+trU3rU2XQchVNZI5lbtsOdo1Z61hMrftXd6fmmA0f1o1fBfKq1Z1RG2PWsbvY3KHqjaSCuMa1cR1xs9evxHjRplRrP3JaRfiFWbrDXInjfzNKDrYInagiOQtAzaPNzzxoTO+uBaRh0kTmer0EH0XFsqeHZtuJjjY92k8GwyrzcG9MaZ3jDTmwE5HfDQdb9a1zmlTeD12Przeqc37XSQUd0/etNKr0/ebnrqzR89nq7dOQAgO/RJB4BLpE2gtdm01qZ4skYxvlBtjPZf1KaxGqo9afNuX+no1BqgdKRlzxBn1Q7pF18dgVhDp46o7Y/tBoO/9pfWput81Tptm/YTdW3qbvVPdqV9W63g5jm1nr/pF3o9Lq7hQp9rANApz1xvxGjto0VvXHjrAqBh35c+wxeio2DrcdDRsz1rG/W5577LCQ02+nfkOv3fmTNnvP5d+fvzWKy5yzds2JBluYbFadOmZZmtwTo+2iLFV3pjT28E6ajjnjXRehz1PMhudHF/0VkE9Lx588033ZZr7biGbWv7Vm2z6/HWY6CtS7wdH8/rzvn+NrWvuW7LdVR9Hcldb1Do3522ZsmuO5E3GsJ1v+rc667l1e1omdu1ayf+otcEfei5odckHWfDW22/3oDS81mnVQSAC6EmHQAukU7LpM2ldeA5rVXSKc30S6/WROly/YKpc4+fj04Zpa/VQdJ0cCMdWE4HdtMB0LSWS//tC/1iq1/8tQZQB0nTgbS076SWSadd0zKpCRMmyLXXXmua6+t2tdZJpzfTPqc6xZW2ELAjf+wvvbmiwUofOse3Z+2aTium76MDAGqfVa2h1inGdH9a/XUDRfsI67Rv2v9ca6q1xl+b3mtws/p761RY2jddm+1rOfUz6LzuGuw8abDX9+jbt6+ZN1trJPXcOB+twR4xYkSW5TrdlIYc/Z1uW8uorRC0FlZrKT/55BPTx9jX0KoDq2lQ1D7KOgWb1dfXau7tWjt7MZ8nJ/T812bVeg55TnU3c+ZMExr1BoVuSwO2hlHtn6/7XW905WSudFd6o2zIkCEyaNAg0zRea1l1UDmdCk77Nuv2PD+X3ljTacv81VVA31/nB9fp0vRY1q1bV5YuXSqffvqp6Qpi3QzSslgtS/RY6fRpegNFP4PnTT49Pnr90XNEW5zoOvp3pP3JtUm43vDQ2ng9bzXY6rRoOm2ba+sUXUcHbnv66afN9Ij6sOiUbtofPDsa6PXc1JtI+j66X7VWXZuk6/mSXXP0S6lNt8737N5bB8rTY3u+cgOAU6iHlwcAO0zBVqBAgSzrWtMj5WRqKp0W6+WXXzZTD+k0S0WLFjXTtw0bNsxx4sSJbKfmcnX48GHzu/LlyztiY2PNFEA6NdGUKVOyTDE1b948t9d6mx5IrVmzxtG6dWtHoUKFzGesU6dOlqnHdu3a5ejcubPZnm63XLlyjltuucUxf/58RyCnYNN95Sm7/ettv+Vkf11Is2bNzHs/9NBDWX6nn1+ncNJpz3Q6pwoVKjgeeeQRx8GDBy96CracHDdr32zYsMHRpEkTR968ec1+0emjPOmxa9WqlXNqr2effdaxbNmyLPv71KlTjnvvvddRpEgR87sLTcemv3edIsv1odNgWT766CPHtddea84tfeh0afq5deoxz8/jyXMqMLV7925z/PPly+coUaKE46mnnjLb0O26TsuX3efx9e/Dm7Fjx7pNxedKl73yyiuOa665xqyj54VOVfj444+7TVV2oanVPL3//vuOf//732Yf6rHU/ajXDtepES16XdFz3V9TsCmdcq5Pnz5mOjH9W9LPNGbMGLfpGtVnn31mriF6TlasWNFc83Q6M8+pAA8dOmSOo1539HfWdGxLly411xZrO/p7/RvUsnpuK7vzz/X9LkT/ZnRf6rb076NHjx5ZptHL6fl5vnNIrwk6NZxOsZadxo0bO+6///4clRsAovQ//4vsAAAglLSmVJvfe2vKH4m0y0afPn1My45AzTTgSptDa4366NGjzYjidqL9xrXFhO6Tnj17hro4+P/071VbfmirCG8zZWgrGG3Kry19tCUOAFwIfdIBAIAteI5yrn14tS++jogdjICudJR/bWI9ZsyYSxqxPBB0/AHdDxczdzwCZ8aMGaaLk3Z9yq57jo5gT0AHkFPUpAMAYCORXJOug5RpX2UNM1qjraOp6xgK2jddp7oC7OTrr782A8Jp7bn269dZIgDAHxg4DgAA2IKO8K6jZGso15pJHRBQB2XzHHkfsIMXXnjBDPKng4XqoJIA4C/UpAMAAAAAYBP0SQcAAAAAwCYI6QAAAAAA2ETE9UnXkVoPHDgghQoVkqioqFAXBwAAAAAQ5hwOh5lKs2zZspInz/nryiMupGtAL1++fKiLAQAAAACIML///rv861//Ou86ERfStQbd2jkJCQmhLg7C2NmzZ2Xp0qXSpk0biY2NDXVxAOCScV0DEG64riFYUlJSTGWxlUdtG9JXrVolY8aMkeTkZDl48KB88skn0qFDhxy99ttvv5XmzZtLrVq1ZPPmzTneptXEXQM6IR2Bvujnz5/fnGdc9AGEA65rAMIN1zUEW066XId04LjTp09L3bp1ZcKECT697u+//5bOnTtLy5YtA1Y2AAAAAACCLaQ16W3btjUPXz366KNy7733SnR0tCxYsCAgZQMAAAAAINhyXZ/0d955R3bv3i3vv/++jBgx4oLrp6WlmYdrXwCraYs+gECxzi/OMwDhgusagHDDdQ3B4ss5lqtC+o4dO2TAgAGyevVqiYnJWdFHjRolw4YNy7JcB4jQ/idAoC1btizURQAAv+K6BiDchMt1Taf2utD0XgicjIwMM9WaN6mpqeEX0vUDaxN3DdzVqlXL8esGDhwoffv2zTKqno7gyMBxCPTdMr3gt27dmoFIAIQFrmsAwk24XNf0cxw+fFj++eefUBdFIn1QuDJlykiBAgWy/M5q0R1WIV0nft+wYYNs2rRJevXqZZZlZmaaOxVaq6414zfeeGOW18XHx5uHJ/0jzM1/iMg9ONcAhBuuawDCTW6+rmkm0u7AOl5XuXLlJC4uLkcjiMO/NJcePXpUDh06JFWrVjXHw5Uv51euCela6/3zzz+7LZs4caJ8/fXXMn/+fKlUqVLIygYAAAAAoZCenm6CurYWpjtvaJUoUUL27t1rWjZ4hnRfhDSknzp1Snbu3Ol8vmfPHjPnebFixaRChQqmqfr+/ftl5syZpm+FzonuqmTJkpI3b94sywEAAAAgktAXPfT81YIhpCFdm6+3aNHC+dzqO56UlCQzZsyQgwcPyr59+0JYQgAAAAAAgiekIf2GG27IdvQ7pUH9fJ5//nnzAAAAAAAgHNAmAgAAAAAQ8qbiCxYs8Pu6uREhHQAAAADg1KVLFxOE9aGjxVepUkVeeOEFOXfuXMC2qV2d27Zt6/d1c6NcM7o7AAAAACA4brrpJnnnnXckLS1NFi1aJD179jTTiOng3p6jy2uQv1SlS5cOyLq5ETXpAAAAAAA38fHxJgxffvnl0qNHD2nVqpV89tlnppa9Q4cO8uKLL0rZsmWlevXqZv3ff/9d7r77bilSpIiZreu2224z05G5mj59ulx11VXmvcuUKSO9evXy2oQ9PT3d/E7X0dm8tAyjRo3yuq7SqbpvvPFGyZcvn1x22WXy8MMPm5nELFaZX3nlFfOeuo7edNCp0uyIkA4AAAAAOC8NwBqe1fLly2Xbtm2ybNkyWbhwoQm7iYmJUqhQIVm9erV8++23UrBgQVMbb73mrbfeMsFYA7SGag382ozem9dff938/sMPPzTb+eCDD6RixYpe1z19+rTZdtGiReWHH36QefPmyVdffeV2A0CtWLFCdu3aZX6+++67ZpDyCw1UHio0dwcAAAAAeKWzcWkoX7JkiTz++ONy9OhRKVCggEybNs3ZzP3999+XzMxMs8yaK1ybymut+sqVK6VNmzYyYsQIeeqpp6R3797O977mmmu8bnPfvn1StWpVufbaa837aU16dmbNmiVnzpyRmTNnmnKpN998U9q3by8vv/yylCpVyizTEK/Lo6OjpUaNGtKuXTvzubp37y52Q006AAAAAMCN1pBrbbg2N9dB2jp27Oic/rp27dpu/dB//PFH2blzp6lJ19foQ5u8a3jW2usjR47IgQMHpGXLljnadpcuXWTz5s2mKf0TTzwhS5cuzXbdLVu2SN26dZ0BXTVr1szcNNBaeIs2s9eAbtFm71ouO6ImHQAAAEBE1hBrv2UdGE1/aq2vVQsMkRYtWpgm6hrGte95TMz/oqNrIFa6/xo0aGCapXsqUaKE5MnjW93w1VdfLXv27JEvv/zSNF3Xvu7aJ37+/PkX/Xl00DtXeqw1yNsRIR0AAABAxNG+zHfddZf59+TJk+XTTz81NcD4XxDPrs+4t1A9d+5cKVmypCQkJHhdR/uUa/NyDf85kZCQYGrv9aHHSfu3//nnn6aG3tWVV15p+pbr8bRuHmifeL0xYA1ql9vQ3B0AAAAAcNHuu+8+KV68uBnRXQeO01pw7YuuTdX/+OMPs442lX/11VfNoHA7duyQjRs3yhtvvOH1/caOHSuzZ8+WrVu3yvbt281gcDrSvLZ28LZtbZKflJQkv/zyixkYTvvOP/DAA87+6LkNIR0AAAAAcNHy588vq1atkgoVKsgdd9xhare7detm+qRbNesaosePHy8TJ040/cNvueUWE9a9KVSokIwePVoaNmxoBpfTqdx0rnZvzeZ12zqonday67pa665933WQuNwqyqGdMSJISkqKFC5cWE6cOJFtUwzAH3QqCr2Y3HzzzVn6wABAbsR1DUA40X7UWvNrya3N3TUIa811pUqVTI0y7HksfMmh1KQDAAAAAGAThHQAAAAAAGyCkA4AAAAAgE0Q0gEAAAAAsAlCOgAAAAAANkFIBwAAAADAJgjpAAAAAADYBCEdAAAAAACbIKQDAAAAAGATMaEuAAAAAADA/xr0nxnU7SWP6ezT+g6HQx555BGZP3++/PXXX1K4cGHp0qWLjB8/XiIZNekAAAAAgKBbvHixzJgxQxYuXCgHDx6UWrVq+X0bN9xwgzz55JNuy44fPy433XSTlC1bVuLj46V8+fLSq1cvSUlJca7z8ccfS+vWraVEiRKSkJAgTZo0kSVLlkgwENIBAAAAAEG3a9cuKVOmjDRt2lRKly4tMTHBaeidJ08eue222+Szzz6T7du3mxsFX331lTz66KPOdVatWmVC+qJFiyQ5OVlatGgh7du3l02bNgW+fAHfAgAAAAAALrRZ++OPPy779u2TqKgoqVixYpZ1/vrrL+ncubMULVpU8ufPL23btpUdO3a41Yh36tRJypUrZ35fu3ZtmT17tts2vvnmG3nttdfMNvSxd+9e8349evSQhg0byuWXXy4tW7aUxx57TFavXu18rTa5f/rpp+Waa66RqlWrysiRI83Pzz//POD7hpAOAAAAAAgqDc4vvPCC/Otf/zJN3X/44Ycs63Tp0kU2bNhgarzXrVtn+rDffPPNcvbsWfP7M2fOSIMGDeSLL76QX375RR5++GF54IEHZP369c5taDP17t27m23oQ5u2ezpw4IBp3t68efNsy5uZmSknT56UYsWKSaAR0gEAAAAAQaWDxBUqVEiio6NNU3ft++1qx44dJpxPmzZNrrvuOqlbt6588MEHsn//flmwYIFZR2vQ+/XrJ/Xq1ZPKlSubmnnta/7hhx86txEXF2dq2XUb+tDtWbQWXn+n76P9znVb2XnllVfk1KlTcvfdd0ugEdIBAAAAALayZcsW00e9cePGzmWXXXaZVK9e3fxOZWRkyPDhw00zd63hLliwoBncTZvQ58S4ceNk48aN8umnn5r+8X379vW63qxZs2TYsGEm/JcsWVICjSnYAAAAAAC5zpgxY0yTdu0/rkG9QIECZiT39PT0HL3eql2vUaOGCflaYz948GAzmJ1lzpw58tBDD8m8efOkVatWEgzUpAMAAAAAbOXKK6+Uc+fOyffff+82UNy2bdukZs2a5vm3335rRmm///77TXN4bfKuo7W70ubuWuN+IdrnXKWlpTmX6SB0Xbt2NT/btWsnwUJNOgAAAADAVqpWrWoCuA76NnnyZNN/fcCAAab/uC631pk/f76sXbvWjNg+duxYOXz4sDPEKx01XoO+juquzeG1xlznZ9f1dOR2Xfbrr79K//79pVmzZs5R5rWJe1JSkqmp1yb3hw4dMsvz5ctn+roHEiEdAAAAAMJQ8pjOkpu988470rt3b7nllltME/brr7/ezFseGxtrfj9o0CDZvXu3JCYmmgHgdHT3Dh06yIkTJ5zvoQPLadjW4P7PP//Inj17TNCeOnWq9OnTx9Sc64jvd9xxh7kJYJkyZYqpye/Zs6d5WPS9dF71QCKkAwAAAACCTvuP68OycuVKt99r7fjMmTOzfb3WilsjvWenWrVqZvo2V1pbrrXv5+NZlmCiTzoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAEAu53A4Ql2EiOfw0zEgpAMAAABALmVNR5aamhrqokS89PR08zM6OvqS3ocp2AAAAAAgl9JAWKRIETly5Ih5rvOFR0VFhbpYESczM1OOHj1q9n9MzKXFbEI6AAAAAORipUuXNj+toI7QyJMnj1SoUOGSb5IQ0gEAAAAgF9NQWKZMGSlZsqScPXs21MWJWHFxcSaoXypCOgAAAACESdP3S+0PjdBj4DgAAAAAAGyCkA4AAAAAgE0Q0gEAAAAAsAlCOgAAAAAANkFIBwAAAADAJgjpAAAAAADYBCEdAAAAAACbIKQDAAAAAGAThHQAAAAAAGyCkA4AAAAAgE0Q0gEAAAAAsImQhvRVq1ZJ+/btpWzZshIVFSULFiw47/off/yxtG7dWkqUKCEJCQnSpEkTWbJkSdDKCwAAAABA2Ib006dPS926dWXChAk5DvUa0hctWiTJycnSokULE/I3bdoU8LICAAAAABBoMRJCbdu2NY+cGj9+vNvzkSNHyqeffiqff/651K9fPwAlBAAAAAAgQkL6pcrMzJSTJ09KsWLFsl0nLS3NPCwpKSnm59mzZ80DCBTr/OI8AxAuuK4BCCee1zLyAQLJl3MrV4f0V155RU6dOiV33313tuuMGjVKhg0blmX50qVLJX/+/AEuISCybNmyUBcBAPyK6xqAcOBakWdd2+Lj40NWHoS31NTUHK8b5XA4HGIDOnDcJ598Ih06dMjR+rNmzZLu3bub5u6tWrXyqSa9fPnycuzYMTP4HBDIu2V6sddxFGJjY0NdHAC4ZFzXAIQTrey76667nM/nz58vBQsWDGmZEL40hxYvXlxOnDhxwRyaK2vS58yZIw899JDMmzfvvAFd6d0wb3fE9MsFXzAQDJxrAMIN1zUA4cDzOsa1DYHky7mV6+ZJnz17tnTt2tX8bNeuXaiLAwAAAACA38SEuonJzp07nc/37NkjmzdvNgPBVahQQQYOHCj79++XmTNnOpu4JyUlyWuvvSaNGzeWQ4cOmeX58uWTwoULh+xzAAAAAADgDyGtSd+wYYOZOs2aPq1v377m30OGDDHPDx48KPv27XOuP2XKFDl37pz07NlTypQp43z07t07ZJ8BAAAAAICwqEm/4YYb5Hzj1s2YMcPt+cqVK4NQKgAAAAAAQiPX9UkHAAAAACBcEdIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgEyEN6atWrZL27dtL2bJlJSoqShYsWHDB16xcuVKuvvpqiY+PlypVqsiMGTOCUlYAAAAAAMI6pJ8+fVrq1q0rEyZMyNH6e/bskXbt2kmLFi1k8+bN8uSTT8pDDz0kS5YsCXhZAQAAAAAItBgJobZt25pHTk2aNEkqVaokr776qnl+5ZVXypo1a2TcuHGSmJjo9TVpaWnmYUlJSTE/z549ax5AoFjnF+cZgHDBdQ1AOPG8lpEPEEi+nFshDem+WrdunbRq1cptmYZzrVHPzqhRo2TYsGFZli9dulTy588fkHICrpYtWxbqIgCAX3FdAxAOXCvyrGubdqkFAiE1NTU8Q/qhQ4ekVKlSbsv0udaO//PPP5IvX74srxk4cKD07dvX+VzXLV++vLRp00YSEhKCUm5E7t0yvdi3bt1aYmNjQ10cALhkXNcAhJNTp07J5MmTnc/12lawYMGQlgnhy2rRHXYh/WLo3TBvd8T0ywVfMBAMnGsAwg3XNQDhwPM6xrUNgeTLuZWrpmArXbq0HD582G2ZPtcacW+16AAAAAAA5Ca5KqQ3adJEli9f7rZMm93pcgAAAAAAcrs8oe4HolOp6cOaYk3/vW/fPmd/8s6dOzvXf/TRR2X37t3y9NNPy9atW2XixIny4YcfSp8+fUL2GQAAAAAACIuQvmHDBqlfv755KB3gTf89ZMgQ8/zgwYPOwK50+rUvvvjC1J7r/Oo6Fdu0adOynX4NAAAAAIDcJKQDx91www3icDiy/f2MGTO8vmbTpk0BLhkAAAAAAMGXq/qkAwAAAAAQzgjpAAAAAADYBCEdAAAAAACbIKQDAAAAAGAThHQAAAAAAGyCkA4AAAAAgE0Q0gEAAAAAsAlCOgAAAAAANkFIBwAAAADAJgjpAAAAAADYBCEdAAAAAACbiPFl5S1btsicOXNk9erV8ttvv0lqaqqUKFFC6tevL4mJiXLnnXdKfHx84EoLAAAAAECk16Rv3LhRWrVqZcL4mjVrpHHjxvLkk0/K8OHD5f777xeHwyHPPfeclC1bVl5++WVJS0sLfMkBAAAAAIjEmnStIe/fv7/Mnz9fihQpku1669atk9dee01effVVefbZZ/1ZTgAAAAAAwl6OQvr27dslNjb2gus1adLEPM6ePeuPsgEAAAAAEFFy1Nz9QgH977//9ml9AAAAAADgh9Hdtc/53Llznc/vvvtuueyyy6RcuXLy448/+vp2AAAAAADgYkP6pEmTpHz58ubfy5YtM48vv/xS2rZta/qtAwAAAACAIEzBpg4dOuQM6QsXLjQ16W3atJGKFSuaUd8BAAAAAECQatKLFi0qv//+u/n34sWLzdRsSqdhy8jIuMhiAAAAAAAAn2vS77jjDrn33nulatWqcvz4cdPMXW3atEmqVKkSiDICAAAAABARfA7p48aNM03btTZ99OjRUrBgQbP84MGD8thjjwWijAAAAAAARASfQ7pOr9avX78sy/v06eOvMgEAAAAAEJFy1Cf9u+++y/Ebpqamyq+//nopZQIAAAAAICLlKKQ/8MADkpiYKPPmzZPTp097Xee///2vPPvss3LFFVdIcnKyv8sJAAAAAEDYy1Fzdw3gb731lgwaNMgMGletWjUpW7as5M2bV/766y/ZunWrnDp1Sm6//XZZunSp1K5dO/AlBwAAAAAgEkO69kN/4oknzGPDhg2yZs0a+e233+Sff/6RunXrmv7oLVq0kGLFigW+xAAAAAAAhCmfB45r2LCheQAAAAAAgBD0SQcAAAAAAIFHSAcAAAAAwCYI6QAAAAAA2AQhHQAAAACAcAjpZ86c8V9JAAAAAACIcD6H9MzMTBk+fLiUK1dOChYsKLt37zbLBw8eLG+//XYgyggAAAAAQETwOaSPGDFCZsyYIaNHj5a4uDjn8lq1asm0adP8XT4AAAAAACKGzyF95syZMmXKFLnvvvskOjraubxu3bqydetWf5cPAAAAAICI4XNI379/v1SpUsVrM/izZ8/6q1wAAAAAAEQcn0N6zZo1ZfXq1VmWz58/X+rXr++vcgEAAAAAEHFifH3BkCFDJCkpydSoa+35xx9/LNu2bTPN4BcuXBiYUgIAAAAAEAF8rkm/7bbb5PPPP5evvvpKChQoYEL7li1bzLLWrVsHppQAAAAAAEQAn2vS1XXXXSfLli3zf2kAAAAAAIhgFxXSLadOnTJN3l0lJCRcapkAAAAAAIhIPjd337Nnj7Rr1840dS9cuLAULVrUPIoUKWJ+AgAAAACAINWk33///eJwOGT69OlSqlQpiYqKushNAwCA3EL/368t6NLS0sxPvTnPdwAAAGwQ0n/88UdJTk6W6tWrB6A4AADAjk6fPi133XWX+ffkyZPl008/lYIFC4a6WAAAhB2fm7tfc8018vvvvwemNAAAAAAARDCfa9KnTZsmjz76qJknvVatWhIbG+v2+zp16vizfAAAAAAARAyfQ/rRo0dl165d0rVrV+cy7ZOmfdX0Z0ZGhr/LCAAAAABARPA5pD/44INSv359mT17NgPHAQAAAAAQypD+22+/yWeffSZVqlTxZzkAAAAAAIh4Pg8cd+ONN5oR3gEAAAAAQIhr0tu3by99+vSRn3/+WWrXrp1l4Lhbb73Vn+UDAAAAACBi+BzSdWR39cILL2T5HQPHAQAAAAAQxJCemZl5CZsDAAAAAAB+65PubxMmTJCKFStK3rx5pXHjxrJ+/frzrj9+/HipXr265MuXT8qXL2+a3p85cyZo5QUAAAAAIKQ16a+//ro8/PDDJkjrv8/niSeeyPHG586dK3379pVJkyaZgK4BPDExUbZt2yYlS5bMsv6sWbNkwIABMn36dGnatKls375dunTpYprZjx07NsfbBQAAAAAg14b0cePGyX333WdCuv47OxqWfQnpGqy7d+8uXbt2Nc81rH/xxRcmhGsY97R27Vpp1qyZ3Hvvvea51sB36tRJvv/++2y3kZaWZh6WlJQU8/Ps2bPmAQSKdX5xngEIB57XMv4/CiC347qGYPLl3MpRSN+zZ4+sWrXK1F7rv/0hPT1dkpOTZeDAgc5lefLkkVatWsm6deu8vka3//7775sm8Y0aNZLdu3fLokWL5IEHHsh2O6NGjZJhw4ZlWb506VLJnz+/Xz4LcD7Lli0LdREA4JK53vC2rm3x8fEhKw8AXCquawim1NRU/w8c16JFCzl48KDXZugX49ixY2Yk+FKlSrkt1+dbt271+hqtQdfXXXvtteJwOOTcuXNmtPlnn3022+3oTQBtUu9ak6592du0aSMJCQl++SxAdnfL9GLfunXrLFMVAkBuc+rUKZk8ebLzuV7bChYsGNIyAcCl4LqGYLJadPs1pGsoDrWVK1fKyJEjZeLEiaYP+86dO6V3794yfPhwGTx4sNfX6N0wb3fENDQRnBAMnGsAwoHndYxrG4DcjusagsmXc8unKdi0z7m/FC9eXKKjo+Xw4cNuy/V56dKlvb5Gg7g2bX/ooYfM89q1a8vp06fNoHbPPfecaS4PAAAAAEBu5VNI15HUL9RP4+OPP87Re8XFxUmDBg1k+fLl0qFDB+cc7Pq8V69e2bbj9wziGvTtUtMPAAAAAEDQQnqhQoXM/OT+on3Fk5KSpGHDhmYgOJ2CTWvGrdHeO3fuLOXKlTODv6n27dubEeHr16/vbO6uteu63ArrAAAAAABEREjXOdL9NXCc6tixoxw9elSGDBkihw4dknr16snixYudg8nt27fPreZ80KBBpsm9/ty/f7+UKFHCBPQXX3zRb2UCAAAAAMD2Id2f/dFdadP27Jq360BxrmJiYmTo0KHmAQAAAABAuMnxSGv0+QYAAAAAwCYhfcWKFVKsWLHAlgYAAAAAgAiW4+buzZs3D2xJAAAAAACIcEwsDgAAAACATRDSAQAAAACwCUI6AAAAAAC5OaTv2rXLzFXeqVMnOXLkiFn25Zdfyq+//urv8gEAAAAAEDF8DunffPON1K5dW77//nv5+OOP5dSpU2b5jz/+yPzlAAAAAAAEM6QPGDBARowYIcuWLZO4uDjn8htvvFG+++67SykLAAAAAAARzeeQ/vPPP8vtt9+eZXnJkiXl2LFj/ioXAAAAAAARx+eQXqRIETl48GCW5Zs2bZJy5cr5q1wAAAAAAEQcn0P6PffcI88884wcOnRIoqKiJDMzU7799lvp16+fdO7cOTClBAAAAAAgAvgc0keOHCk1atSQ8uXLm0HjatasKddff700bdrUjPgOAAAAAAAuToyvL9DB4qZOnSpDhgwx/dM1qNevX1+qVq16kUUAAAAAAAAXFdItWpOuDwAAAAAAEKKQfuedd0qjRo1Mv3RXo0ePlh9++EHmzZvnp6IBAAAAwdeg/8xQFwFBEHUuXQq7PL9h8BxxxPxvimmEp+QxncOvT/qqVavk5ptvzrK8bdu25ncAAAAAACBIIV37oGu/dE+xsbGSkpJykcUAAAAAAAA+h/TatWvL3LlzsyyfM2eOGekdAAAAAAAEqU/64MGD5Y477pBdu3bJjTfeaJYtX75cZs+eTX90AAAAAACCGdLbt28vCxYsMPOlz58/X/Llyyd16tSRr776Spo3b34pZQEAAAAAIKJd1BRs7dq1Mw8AAAAAAGCDedLT09PlyJEjkpmZ6ba8QoUK/igXAAAAAAARx+eQvmPHDnnwwQdl7dq1bssdDodERUVJRkaGP8sHAAAAAEDE8Dmkd+nSRWJiYmThwoVSpkwZE8wBAAAAAEAIQvrmzZslOTlZatSo4YfNAwAAAACAi54nXedCP3bsmK8vAwAAAAAA/g7pL7/8sjz99NOycuVKOX78uKSkpLg9AAAAAABAkJq7t2rVyvxs2bKl23IGjgMAAAAAIMghfcWKFYEpCQAAAAAAEc7nkN68efPAlAQAAAAAgAjnc590tXr1arn//vuladOmsn//frPsvffekzVr1vi7fAAAAAAARAyfQ/pHH30kiYmJki9fPtm4caOkpaWZ5SdOnJCRI0cGoowAAAAAAEQEn0P6iBEjZNKkSTJ16lSJjY11Lm/WrJkJ7QAAAAAAIEghfdu2bXL99ddnWV64cGH5+++/L7IYAAAAAADA55BeunRp2blzZ5bl2h+9cuXK/ioXAAAAAAARx+eQ3r17d+ndu7d8//33Zl70AwcOyAcffCD9+vWTHj16BKaUAAAAAABEAJ+nYBswYIBkZmZKy5YtJTU11TR9j4+PNyH98ccfD0wpAQAAAACIAD6F9IyMDPn222+lZ8+e0r9/f9Ps/dSpU1KzZk0pWLBg4EoJAAAAAEAE8CmkR0dHS5s2bWTLli1SpEgRE84BAAAAAECI+qTXqlVLdu/e7afNAwAAAACAS5onXfufL1y4UA4ePCgpKSluDwAAAAAAEKSB426++Wbz89ZbbzWju1scDod5rv3WAQAAAABAEEL6ihUrLmIzAAAAAADA7yG9efPmvr4EAAAAAAAEok+6Wr16tdx///3StGlT2b9/v1n23nvvyZo1ay7m7QAAAAAAwMWE9I8++kgSExMlX758snHjRklLSzPLT5w4ISNHjgxEGQEAAAAAiAgXNbr7pEmTZOrUqRIbG+tc3qxZMxPaAQAAAABAkEL6tm3b5Prrr8+yvHDhwvL3339fZDEAAAAAAIDPIb106dKyc+fOLMu1P3rlypX9VS4AAAAAACKOzyG9e/fu0rt3b/n+++/NvOgHDhyQDz74QPr16yc9evQITCkBAAAAAIgAPk/BNmDAAMnMzJSWLVtKamqqafoeHx9vQvrjjz8emFICAAAAABABchTSf/rpJ6lVq5bkyZPH1J4/99xz0r9/f9Ps/dSpU1KzZk0pWLBg4EsLAAAAAECkN3evX7++HDt2zPxb+50fP35c4uLiTDhv1KgRAR0AAAAAgGCF9CJFisiePXvMv/fu3Wuau/vLhAkTpGLFipI3b15p3LixrF+//rzr6wjyPXv2lDJlyphm9tWqVZNFixb5rTwAAAAAANi6ufudd94pzZs3N8FYm7s3bNhQoqOjva67e/fuHG987ty50rdvXzPvugb08ePHS2JiopnmrWTJklnWT09Pl9atW5vfzZ8/X8qVKye//fabuYkAAAAAAEBEhPQpU6bIHXfcYfqgP/HEE2aE90KFCl3yxseOHWveq2vXrua5hvUvvvhCpk+fbgao86TL//zzT1m7dq3ExsaaZVoLDwAAAABARI3uftNNN5mfycnJZgq2Sw3pWiuu7zVw4EDnMh2YrlWrVrJu3Tqvr/nss8+kSZMmprn7p59+KiVKlJB7771XnnnmmWxr9tPS0szDkpKSYn6ePXvWPIBAsc4vzjMA4cDzWsb/RxHO4rx/rUS4cbg/jdXjzrEPe2dD9P8uX7br8xRs77zzjviDDkSXkZEhpUqVcluuz7du3ZptU/qvv/5a7rvvPtMPXWv2H3vsMfOBhw4d6vU1o0aNkmHDhmVZvnTpUsmfP79fPgtwPsuWLQt1EQDgkrne8LaubTo2DBCOBjQpHOoiIEjXtcmb/ve8T6MErmsRYFGIxjPT6csDFtJPnz4tL730kixfvlyOHDmSZRA5X/qk+0q3pf3Rtfm91pw3aNBA9u/fL2PGjMk2pGtNvfZ7d61JL1++vLRp00YSEhICVlZAbx7pl1gdR8HqngEAuZVOuTp58mTnc722MbsLwtX1g2eHuggIhnPp4lplN259ikhMXAgLhGBYNbxTSLZrtegOSEh/6KGH5JtvvpEHHnjAOZDcxShevLgJ2ocPH3Zbrs9Lly7t9TW6PQ07rk3br7zySjl06JBpPq/TwnnSu2He7ojp+xCcEAycawDCged1jGsbwll6RqhLgGCI8jjOZzNEHBcXbZCLxIbo/12+bNfnkP7ll1+awd2aNWsml0IDtdaEa418hw4dnDXl+rxXr15eX6PbnDVrlllP+6+r7du3m/DuLaADAAAAABB286S7Klq0qBQrVswvG9dm6FOnTpV3331XtmzZIj169DDN6a3R3jt37uw2sJz+Xkd314HrNJzrzYKRI0eageQAAAAAAMjtfK5JHz58uAwZMsQE60sdeK1jx45y9OhR837aZL1evXqyePFi52By+/btc9aYK+1LvmTJEunTp4/UqVPHzJOugV1HdwcAAAAAIOJC+quvviq7du0yQVrnKPdsW79x40af3k+btmfXvH3lypVZlukUbN99952PpQYAAAAAIAxDutV/HAAAAAAAhDikZzfVGQAAAAAACPLAcQAAAAAAIMQ16Tqqe07mRNfR1wEAAAAAQABD+vjx4y/i7QEAAAAAgN9DelJSUo7fFAAAAAAA+I4+6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAEBuG93d0rdvX6/LdQ71vHnzSpUqVeS2226TYsWK+aN8AAAAAABEDJ9D+qZNm2Tjxo2SkZEh1atXN8u2b98u0dHRUqNGDZk4caI89dRTsmbNGqlZs2YgygwAAAAAQFjyubm71pK3atVKDhw4IMnJyebxxx9/SOvWraVTp06yf/9+uf7666VPnz6BKTEAAAAAAGHK55A+ZswYGT58uCQkJDiXFS5cWJ5//nkZPXq05M+fX4YMGWLCOwAAAAAACGBIP3HihBw5ciTL8qNHj0pKSor5d5EiRSQ9Pd3XtwYAAAAAIKJdVHP3Bx98UD755BPTzF0f+u9u3bpJhw4dzDrr16+XatWqBaK8AAAAAACELZ8Hjps8ebLpb37PPffIuXPn/u9NYmIkKSlJxo0bZ57rAHLTpk3zf2kBAAAAAAhjPof0ggULytSpU00g3717t1lWuXJls9xSr149/5YSAAAAAIAI4HNz9/fff19SU1NNKK9Tp455uAZ0AAAAAAAQpJCuTd1Lliwp9957ryxatMjMlw4AAAAAAEIQ0g8ePChz5syRqKgoufvuu6VMmTLSs2dPWbt2rR+KAwAAAABA5PI5pOsgcbfccot88MEHZio27Zu+d+9eadGihVxxxRWBKSUAAAAAABHA54HjXOXPn18SExPlr7/+kt9++022bNniv5IBAAAAABBhfK5JVzpwnNak33zzzVKuXDkZP3683H777fLrr7/6v4QAAAAAAEQIn2vSdX70hQsXmlp07ZM+ePBgadKkSWBKBwAAAABABPE5pEdHR8uHH35omrnrv1398ssvUqtWLX+WDwAAAACAiOFzSNdm7q5Onjwps2fPlmnTpklycjJTsgEAAAAAEMw+6WrVqlWSlJRkpmB75ZVX5MYbb5TvvvvuYt8OAAAAAICI51NN+qFDh2TGjBny9ttvS0pKiumTnpaWJgsWLJCaNWsGrpQAAAAAAESAHNekt2/fXqpXry4//fSTGc39wIED8sYbbwS2dAAAAAAARJAc16R/+eWX8sQTT0iPHj2katWqgS0VAAAAAAARKMc16WvWrDGDxDVo0EAaN24sb775phw7diywpQMAAAAAIILkOKT/+9//lqlTp8rBgwflkUcekTlz5kjZsmUlMzNTli1bZgI8AAAAAAAI4ujuBQoUkAcffNDUrP/888/y1FNPyUsvvSQlS5aUW2+99RKKAgAAAABAZLvoKdiUDiQ3evRo+eOPP8xc6QAAAAAAIEQh3RIdHS0dOnSQzz77zB9vBwAAAABARPJpnnQAADw16D8z1EVAEESdS5fCLs9vGDxHHDFxISwRgiF5TOdQFwEAIo5fatIBAAAAAMClI6QDAAAAAGAThHQAAAAAAGyCkA4AAAAAgE0Q0gEAAAAAsAlCOgAAAAAANkFIBwAAAADAJgjpAAAAAADYBCEdAAAAAACbIKQDAAAAAGAThHQAAAAAAGyCkA4AAAAAgE0Q0gEAAAAAsAlCOgAAAAAANkFIBwAAAADAJgjpAAAAAADYhC1C+oQJE6RixYqSN29eady4saxfvz5Hr5szZ45ERUVJhw4dAl5GAAAAAADCPqTPnTtX+vbtK0OHDpWNGzdK3bp1JTExUY4cOXLe1+3du1f69esn1113XdDKCgAAAABAWIf0sWPHSvfu3aVr165Ss2ZNmTRpkuTPn1+mT5+e7WsyMjLkvvvuk2HDhknlypWDWl4gJxwOh5w6dUrS0tLMT30OAAAAABcSIyGUnp4uycnJMnDgQOeyPHnySKtWrWTdunXZvu6FF16QkiVLSrdu3WT16tXn3YaGJH1YUlJSzM+zZ8+aBxAIGszvuusu8+/JkyfL/PnzpWDBgqEuFhAQcdGhLgGCwuNeY6wed4592IvU70pc1yIE17WIdDZE1zVfthvSkH7s2DFTK16qVCm35fp869atXl+zZs0aefvtt2Xz5s052saoUaNMjbunpUuXmhp7IBBcbwypZcuWSXx8fMjKAwTSgCaFQ10EBOm6NnnT/573aZTAdS0CLFq0SCIR17XIwHUtMi0K0XUtNTU1d4R0X508eVIeeOABmTp1qhQvXjxHr9Faeu3z7lqTXr58eWnTpo0kJCQEsLSI9Jp0rUG3tG7dmpp0hK3rB88OdREQDOfSxfXW9rj1KSIxcSEsEIJh1fBOEom4rkUIrmsRaVWIrmtWi27bh3QN2tHR0XL48GG35fq8dOnSWdbftWuXGTCuffv2zmWZmZnmZ0xMjGzbtk2uuOIKt9fo3TBvd8RiY2PNAwgEz3OL8w3hLD0j1CVAMER5HOezGSKOqFCVBsESqf/v4roWGbiuRabYEF3XfNluSAeOi4uLkwYNGsjy5cvdQrc+b9KkSZb1a9SoIT///LNp6m49br31VmnRooX5t9aQAwAAAACQW4W8ubs2RU9KSpKGDRtKo0aNZPz48XL69Gkz2rvq3LmzlCtXzvQt13nUa9Wq5fb6IkWKmJ+eywEAAAAAyG1CHtI7duwoR48elSFDhsihQ4ekXr16snjxYudgcvv27TMjvgMAAAAAEO5CHtJVr169zMOblStXnve1M2bMCFCpAAAAAAAILqqoAQAAAACwCUI6AAAAAAA2QUgHAAAAAMAmCOkAAAAAANgEIR0AAAAAAJsgpAMAAAAAYBOEdAAAAAAAbIKQDgAAAACATRDSAQAAAACwCUI6AAAAAAA2QUgHAAAAAMAmCOkAAAAAANgEIR0AAAAAAJsgpAMAAAAAYBOEdAAAAAAAbIKQDgAAAACATRDSAQAAAACwCUI6AAAAAAA2QUgHAAAAAMAmCOkAAAAAANgEIR0AAAAAAJsgpAMAAAAAYBOEdAAAAAAAbIKQDgAAAACATRDSAQAAAACwCUI6AAAAAAA2QUgHAAAAAMAmCOkAAAAAANgEIR0AAAAAAJsgpAMAAAAAYBOEdAAAAAAAbIKQDgAAAACATRDSAQAAAACwiZhQFwAAAAAAgs0RHSup9TtJn0YJMm59ijgkNtRFAgxq0gEAAABEnqgokZg4iY+PNz/Nc8AGCOkAAAAAANgEIR0AAAAAAJsgpAMAAAAAYBOEdAAAAAAAbIKQDgAAAACATRDSAQAAAACwCUI6AAAAAAA2QUgHAAAAAMAmYkJdgEjToP/MUBcBQRB1Ll0Kuzy/YfAcccTEhbBECIbkMZ1DXQQAAADkctSkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABswhYhfcKECVKxYkXJmzevNG7cWNavX5/tulOnTpXrrrtOihYtah6tWrU67/oAAAAAAOQWIQ/pc+fOlb59+8rQoUNl48aNUrduXUlMTJQjR454XX/lypXSqVMnWbFihaxbt07Kly8vbdq0kf379we97AAAAAAAhFVIHzt2rHTv3l26du0qNWvWlEmTJkn+/Pll+vTpXtf/4IMP5LHHHpN69epJjRo1ZNq0aZKZmSnLly8PetkBAIgUjuhYSa3fSR555BHzU58DAAD/i5EQSk9Pl+TkZBk4cKBzWZ48eUwTdq0lz4nU1FQ5e/asFCtWzOvv09LSzMOSkpJifupr9BFscdFB3yRCweH+NFaPO8c+7IXimmIHXNciRZTERcdJfHy8xMXHiWSEujwIBq5riJRjzTGPHGdDdF3zZbshDenHjh2TjIwMKVWqlNtyfb5169YcvcczzzwjZcuWNcHem1GjRsmwYcOyLF+6dKmpsQ+2AU0KB32bCD69MTR50/+e92mUYL7YIrwtWrRIIhHXtcjTtxHHPFJwXUOk4LoWORaF6Lqmlcu5IqRfqpdeeknmzJlj+qnroHPeaC299nl3rUm3+rEnJCRIsF0/eHbQt4kQOJcurreAxq1PEYmJC2GBEAyrhneSSMR1LXJoTZN+kR27/oSkU5MeEbiuIdxxXYs8q0J0XbNadNs+pBcvXlyio6Pl8OHDbsv1eenSpc/72ldeecWE9K+++krq1KmT7Xpae+mtBjM2NtY8go0//sgQ5XGcz2aIOKJCVRoESyiuKXbAdS0yjznHPTJwXUOk4LoWOWJDdF3zZbshHTguLi5OGjRo4DbomzUIXJMmTbJ93ejRo2X48OGyePFiadiwYZBKCwAAAABAYIW8ubs2RU9KSjJhu1GjRjJ+/Hg5ffq0Ge1dde7cWcqVK2f6lquXX35ZhgwZIrNmzTJzqx86dMgsL1iwoHkAAAAAAJBbhTykd+zYUY4ePWqCtwZunVpNa8itweT27dtnRny3vPXWW2ZU+LvuusvtfXSe9eeffz7o5QcAAAAAIGxCuurVq5d5eKODwrnau3dvkEoFAAAAAEBwhbRPOgAAAAAA+B9COgAAAAAANkFIBwAAAADAJgjpAAAAAADYBCEdAAAAAACbIKQDAAAAAGAThHQAAAAAAGyCkA4AAAAAgE0Q0gEAAAAAsAlCOgAAAAAANkFIBwAAAADAJgjpAAAAAADYREyoCwCEI0d0rKTW7yR9GiXIuPUp4pDYUBcJAAAAQC5ATToQCFFRIjFxEh8fb36a5wAAAABwAYR0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAALAJQjoAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE7YI6RMmTJCKFStK3rx5pXHjxrJ+/frzrj9v3jypUaOGWb927dqyaNGioJUVAAAAAICwDelz586Vvn37ytChQ2Xjxo1St25dSUxMlCNHjnhdf+3atdKpUyfp1q2bbNq0STp06GAev/zyS9DLDgAAAABAWIX0sWPHSvfu3aVr165Ss2ZNmTRpkuTPn1+mT5/udf3XXntNbrrpJunfv79ceeWVMnz4cLn66qvlzTffDHrZAQAAAADwpxgJofT0dElOTpaBAwc6l+XJk0datWol69at8/oaXa4176605n3BggVe109LSzMPy4kTJ8zPP//8U86ePSvBlufcP0HfJkIjj0MkNTXWHPM8GaEuDYLh+PHjEom4rkUOrmuRh+sawh3XtchzPETXtZMnT5qfDofD3iH92LFjkpGRIaVKlXJbrs+3bt3q9TWHDh3yur4u92bUqFEybNiwLMsrVap0SWUHcmJtqAuAoCo+/tFQFwEIOK5rkYXrGiIB17XIUjzE1zUN64ULF7ZvSA8GraV3rXnPzMw0teiXXXaZREVFhbRsCG8pKSlSvnx5+f333yUhISHUxQGAS8Z1DUC44bqGYNEadA3oZcuWveC6IQ3pxYsXl+joaDl8+LDbcn1eunRpr6/R5b6sHx8fbx6uihQpcsllB3JKL/hc9AGEE65rAMIN1zUEw4Vq0G0xcFxcXJw0aNBAli9f7lbTrc+bNGni9TW63HV9tWzZsmzXBwAAAAAgtwh5c3dtip6UlCQNGzaURo0ayfjx4+X06dNmtHfVuXNnKVeunOlbrnr37i3NmzeXV199Vdq1aydz5syRDRs2yJQpU0L8SQAAAAAAyOUhvWPHjnL06FEZMmSIGfytXr16snjxYufgcPv27TMjvluaNm0qs2bNkkGDBsmzzz4rVatWNSO716pVK4SfAshKu1kMHTo0S3cLAMituK4BCDdc12BHUY6cjAEPAAAAAAACLqR90gEAAAAAwP8Q0gEAAAAAsAlCOgAAAAAANkFIBwAAAADAJgjpgJ89//zzEhUV5faoUaNGqIsFADm2atUqad++vZQtW9Zcw3QWFVc65qzOylKmTBnJly+ftGrVSnbs2BGy8gLApV7XunTpkuX720033RSy8iKyEdKBALjqqqvk4MGDzseaNWtCXSQAyLHTp09L3bp1ZcKECV5/P3r0aHn99ddl0qRJ8v3330uBAgUkMTFRzpw5E/SyAoA/rmtKQ7nr97fZs2cHtYyAbeZJB8JRTEyMlC5dOtTFAICL0rZtW/PwRmvRx48fL4MGDZLbbrvNLJs5c6aUKlXK1Ezdc889QS4tAFzadc2ic6Xz/Q12QE06EADa7FObU1WuXFnuu+8+2bdvX6iLBAB+sWfPHjl06JBp4m4pXLiwNG7cWNatWxfSsgHApVi5cqWULFlSqlevLj169JDjx4+HukiIUIR0wM/0i+qMGTNk8eLF8tZbb5kvtNddd52cPHky1EUDgEumAV1pzbkrfW79DgByG23qrq2Cli9fLi+//LJ88803puY9IyMj1EVDBKK5O+Bnrk2p6tSpY0L75ZdfLh9++KF069YtpGUDAABAVq5ddWrXrm2+w11xxRWmdr1ly5YhLRsiDzXpQIAVKVJEqlWrJjt37gx1UQDgkln9NQ8fPuy2XJ/TlxNAuNAui8WLF+f7G0KCkA4E2KlTp2TXrl1mqiIAyO0qVapkwrg2CbWkpKSYUd6bNGkS0rIBgL/88ccfpk86398QCjR3B/ysX79+Zh5ObeJ+4MABGTp0qERHR0unTp1CXTQAyPHNRdfaIx1bY/PmzVKsWDGpUKGCPPnkkzJixAipWrWqCe2DBw82g2V26NAhpOUGgIu5rulj2LBhcuedd5qbkFq58vTTT0uVKlXM9JJAsEU5dC4VAH7t07Rq1Spz97VEiRJy7bXXyosvvmj6NQFAbqB9MFu0aJFleVJSkhkYU7866A3IKVOmyN9//22ucxMnTjRdewAgt13XdKBfvcm4adMmc03Tm45t2rSR4cOHZxkkEwgGQjoAAAAAADZBn3QAAAAAAGyCkA4AAAAAgE0Q0gEAAAAAsAlCOgAAAAAANkFIBwAAAADAJgjpAAAAAADYBCEdAAAAAACbIKQDAAAAAGAThHQAAGxo5cqVEhUVJX///bd5PmPGDClSpIiEq+eff17q1at3Ua994IEHZOTIkZKb+XJ8Fy9ebPZVZmZmwMsFAAg+QjoAwLa6dOkiHTp0CPp2fQ3E//zzjxQrVkyKFy8uaWlpASlTx44dZfv27SELwv6kNx8WLFjgl/f68ccfZdGiRfLEE09IpLjpppskNjZWPvjgg1AXBQAQAIR0AAAu0UcffSRXXXWV1KhRw2/h01O+fPmkZMmSAXnv3OyNN96Q//znP1KwYEGJtBtYr7/+eqiLAQAIAEI6ACDXuOGGG0yN6dNPP21qrkuXLm1qhz1rad966y1p27atCbaVK1eW+fPnZ9uMXG3evNks27t3r/l9165d5cSJE2aZPjy34entt9+W+++/3zz03670PfU9dBsW3bYu021ZtDa4WrVqpswtWrQwr7tQ7b5+ziuuuELi4uKkevXq8t5778ml+P333+Xuu+8229H9e9ttt7mVw2rZ8Morr0iZMmXksssuk549e8rZs2ed6xw8eFDatWtnPkelSpVk1qxZUrFiRRk/frz5vf5b3X777WYfWM8t+hl0WeHCheWee+6RkydPZlvejIwMc2zbt2/vtnzixIlStWpVyZs3r5QqVUruuusu5++0ifioUaNM2bSMdevWdTs/1K+//iq33HKLJCQkSKFCheS6666TXbt2OV//wgsvyL/+9S+Jj483LRO0+bnn8f7444/NccyfP7/Zxrp167IczwoVKpjf6744fvx4lhYC+nrdvpajQYMGsmHDBufv9TPrc6tcAIDwQUgHAOQq7777rhQoUEC+//57GT16tAlMy5Ytc1tn8ODBcuedd5qgc99995mwt2XLlhy9f9OmTU2g1GCkgVMf/fr1y3Z9DUkawDTc6mP16tXy22+/+RyO77jjDhO8NMw/9NBDMmDAgPO+5pNPPpHevXvLU089Jb/88os88sgj5ubCihUr5GJo0E5MTDShUD/Dt99+a2qntWl1enq6cz19f/3M+lOPhYZNfVg6d+4sBw4cMDcgtIXBlClT5MiRI87f//DDD+bnO++8Y/at9Vzp+2pLhIULF5rHN998Iy+99FK2Zf7pp5/MzZSGDRs6l2lw1Rs5el5s27bNBOjrr7/e+XsN6DNnzpRJkyaZMN6nTx9zc0W3pfbv32/W1wD+9ddfS3Jysjz44INy7tw58/vXXntNXn31VXOjQrev++zWW2+VHTt2uJXtueeeM+eNHk+9+dKpUyfne+i5261bN+nVq5f5vYbxESNGuL1ez1u9EaD7R8ug54M2cbdowNcbEHqsAABhxgEAgE0lJSU5brvtNufz5s2bO6699lq3da655hrHM88843yu/2t79NFH3dZp3Lixo0ePHubfK1asMOv89ddfzt9v2rTJLNuzZ495/s477zgKFy6cozI+++yzjg4dOjifa3mHDh3qfK7vqe+t27DotnWZlkUNHDjQUbNmTbf31c/kWk7PMjVt2tTRvXt3t9f85z//cdx8883ZllXLVbduXa+/e++99xzVq1d3ZGZmOpelpaU58uXL51iyZInzeFx++eWOc+fOuW2zY8eO5t9btmwxZf7hhx+cv9+xY4dZNm7cOOcyff7JJ59kKVv+/PkdKSkpzmX9+/c3xy47+h7R0dFuZf7oo48cCQkJbu9jOXPmjNnG2rVr3ZZ369bN0alTJ+exqFSpkiM9Pd3rNsuWLet48cUXs5yDjz32mNvxnjZtmvP3v/76q1mm+0fptjyPk+5D1+NbqFAhx4wZMxznU79+fcfzzz9/3nUAALkPNekAgFylTp06bs+12bVrTa1q0qRJluc5rUn3hTa31tpkrYm16L+1ZtmXkbe1bI0bNz7vZ/D2mmbNmrkt0+cX+zm11cHOnTtNTbrWoOtDm7yfOXPGrUm19r2Pjo72uv+15jomJkauvvpq5++rVKkiRYsWzVEZtJm7bt/be2c3YJ/WeGvzckvr1q3l8ssvN90cdNR3HVwtNTXV/E4/n/5b17E+oz60Zt36jFqzrc3bXWutLSkpKaaVQE72u+t5qp9DWZ8lJ8e7b9++pkVFq1atTGsCb83atbm+9dkAAOEjJtQFAADAF57hSQOaL4E4T57/uz/9fxW6/8e1T7UvlixZYppH68jrnuF9+fLlJgz6c3uBdOrUKdPv2duI4SVKlPDb/j8fX99bR9PXkKrN8bVfvtKQv3HjRtPcfunSpTJkyBAzpoA2G9fPqL744gspV66c23tp2LeCr78/i3UTwZf9pGW+9957TVm//PJLGTp0qMyZM8f0X7f8+eefbscGABAeqEkHAISd7777LsvzK6+80vzbCjXaH9riOqib0sCnQftCdJA47e+ur3d96DJrALmcbE/Ltn79+vN+Bk/6Gu037kqf16xZUy6G1n5rv2odQV5rv10fOohbTujgddrvetOmTc5lWnv9119/ZQmwOdm/F2JNJ/ff//7XbbnW5msNtI5ZoP3GdTA37V+u+0bD+L59+7J8xvLlyztrwLWft7cbKTpOQdmyZS95v+ux037pFzre2pdd+8zrzQYds0D78VusFg7169fP8XYBALkDNekAgLAzb948M5jYtddea2qGNQBbodkKZFpT+eKLL5q5x3UgMM9m11rrqrXhOjK3jsCtD1dHjx6Vzz//XD777DOpVauW2+908DSt8dSaTm0y/u9//9s0WdYRxbXJ86BBg9zWf/TRR00Z+vfvb5o460BhroOxeaPr6kB1GtI0kGpZdETxr7766ryv0ybinjcJtPZZByobM2aMGdHdGr1cB8DT99TR9PX5hegUdFqWhx9+2Iw8r2FcB7bT2mnXJum6f3XfajNxDc05bQ7vSW+A6M2FNWvWOAO7Dji3e/duM/ibvq+Omq812HoDQT+nDuamwVeX6fmhA89pyNYAnpSUZAZz02nd9EbLwIEDzQ0KDdCNGjUy76H7XWu1dVR93aYGZ92fvsxZrgPb6WfXwed0f2uLDNcR4vUY6XZ0VHo9Z/744w/TEkAHQ7RomXTfXahbBAAg96EmHQAQdoYNG2aaBmutqPY3nj17trOmU4OjPt+6dav5/csvv5xlZG0d4V2DszZj1yCoNbKe9H11lPmWLVtm+Z0u02D6/vvvm+fTp083NczanPzJJ5/Msj0dqVtHQteRzfWmgI48PnLkyPN+Rp0KTUca16Cn/cQnT55sAqNOU3c+elNCg73rQ0eG15sQq1atMmXRWlut7dURyLXGVgNsTul+0VHHNSTrjYru3bubcKzToVn0hoSOyK83Sy61JlhvargGZJ0+Tm8s3HjjjeYz6L7U4637SA0fPtyM/q+jvOvvdfR6bVKuYVjptHJa6643aZo3b26O2dSpU53N1zVga39xvflQu3ZtE671Ro1O+ZZTetNG31OPnx5vrSl3vXGjff51Sja92aO16XozRqcU1PPaop9Jb6x43jwCAOR+UTp6XKgLAQCAv2iNrU5PpiEWoae1wBrGtYbf2w2NS6W1zlrDPXfu3IipVT527Jj5zDrdnHVzAQAQPmjuDgAA/MaqhdZaZu2Hr03ltXm761zl/qQtFrT2XoNrpNA+9hMnTiSgA0CYIqQDAAC/0QHXnn32WdMvXJu5a9cBbY7ubUozf7lQE/9wo+Mt6AMAEJ5o7g4AAAAAgE0wcBwAAAAAADZBSAcAAAAAwCYI6QAAAAAA2AQhHQAAAAAAmyCkAwAAAABgE4R0AAAAAABsgpAOAAAAAIBNENIBAAAAABB7+H//j4Phb8ZFsgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Best configuration: {'batch_size': 1, 'input_sec': 5, 'precision': 'float32', 'inference_time': 0.145200777053833}\n"
     ]
    }
   ],
   "source": [
    "# === Visualization ===\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df, x='input_sec', y='inference_time', hue='precision')\n",
    "plt.title(\"Inference Time vs Input Length (CPU, float32 only)\")\n",
    "plt.xlabel(\"Input Audio Length (seconds)\")\n",
    "plt.ylabel(\"Avg Inference Time (s)\")\n",
    "plt.legend(title=\"Precision\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "best_config = min(results, key=lambda x: x['inference_time'])\n",
    "print(\"🔥 Best configuration:\", best_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvaakai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
